{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import re\n",
    "import contractions\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, random_split \n",
    "from torch import nn\n",
    "import torch \n",
    "\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print('device = ', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_b8b8f20a\n"
     ]
    }
   ],
   "source": [
    "# root = os.getcwd()\n",
    "# dataset =  os.path.join(root, 'dataset', 'amazon_full')\n",
    "# #yelp polarity has 2 labels, \n",
    "# #yelp dataset has 5 labels, they have the same text content\n",
    "# train_datapath = os.path.join(dataset, 'train.csv') \n",
    "# test_datapath = os.path.join(dataset, 'test.csv') \n",
    "\n",
    "# processed_train_dataset = os.path.join(dataset, 'processed_train_datset.pickle')\n",
    "# processed_test_dataset = os.path.join(dataset, 'processed_test_dataset.pickle')\n",
    "# vocab_save_path = os.path.join(dataset, 'vocab.pickle')\n",
    "\n",
    "ROOT = os.getcwd()\n",
    "DATASET_PATH = os.path.join(ROOT, 'dataset', 'yelp_full' )\n",
    "\n",
    "CLEANED_TRAIN_DATAPATH = os.path.join(DATASET_PATH, 'cleaned_train_20%.csv')\n",
    "CLEANED_TEST_DATAPATH = os.path.join(DATASET_PATH, 'cleaned_test_10%.csv')\n",
    "\n",
    "# model_save_root = os.path.join(root, 'output', 'transformer')\n",
    "\n",
    "OUTPUT_PATH = os.path.join(ROOT, 'output', 'transformer')\n",
    "\n",
    "if os.path.exists(OUTPUT_PATH) == False:\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "id = str(uuid.uuid4()).split('-')[0]\n",
    "EXP_NAME = f'baseline_{id}'\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "print(EXP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test string special characters punctuations _'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning Text\n",
    "\n",
    "def remove_urls(text):\n",
    "    #if there's link in text, like www.something.com, https://www.something.com,\n",
    "    # replace it with the <url> token\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text = pattern.sub(' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_digits(text):\n",
    "    return re.sub(\"\\d\", ' ', text)\n",
    "\n",
    "def remove_punctation(text):\n",
    "    return re.sub(r'[^\\w\\s]',' ',text)\n",
    "\n",
    "def expand_contraction(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split(' ') if word not in STOPWORDS])\n",
    "\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    extract feature and label from line and process the text\n",
    "    @params:\n",
    "        text: string, format: __label__2 some text.\n",
    "    @return:\n",
    "        feature: string\n",
    "        label: int, 0: bad review, 1 good review\n",
    "    '''\n",
    "    #Each line has format: __label__2 some text.\n",
    "    #The first part is label, the rest is text feature\n",
    "    #lower case the features\n",
    "    text = text.lower()\n",
    "    #start cleaning\n",
    "\n",
    "    #remove urls in text\n",
    "    text = remove_urls(text)\n",
    "    #remove digits\n",
    "    text = remove_digits(text)\n",
    "    # # #expand contractions\n",
    "    text = expand_contraction(text)\n",
    "    # # #remove punctuations\n",
    "    text = remove_punctation(text)\n",
    "    # # #remove stop words\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "    #after cleaning, there's a letter n that occur most frequently\n",
    "    #this don't make sense so remove a standalone letter n\n",
    "    text = ' '.join(t for t in text.split() if t != '' and t != 'n')\n",
    "    return text.strip()\n",
    "\n",
    "test_string = '''This is a test string. Here are some special characters: &,#,$. How about some punctuations? !@#$%^&*()_+=-`~{[]}|:;'<,>.?/\"|https://www.example.com'''\n",
    "\n",
    "clean_text(test_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(CLEANED_TRAIN_DATAPATH)\n",
    "NUM_CLASSES = len(train_df['review'].unique())\n",
    "test_df = pd.read_csv(CLEANED_TEST_DATAPATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n",
    "\n",
    "def build_array(X, maxlength = 800):\n",
    "    X_tokens = []\n",
    "    X_lengths = []\n",
    "    max_len = 0\n",
    "    for text in tqdm(X):\n",
    "        tokens = tokenizer(text)[:maxlength]\n",
    "        max_len = max(max_len, len(tokens))\n",
    "        X_tokens.append(tokens)\n",
    "        X_lengths.append(len(tokens))\n",
    "\n",
    "    print('max len = ', max_len)\n",
    "    return X_tokens, X_lengths\n",
    "\n",
    "\n",
    "def get_ids(tokens, vocab):\n",
    "    ids = vocab.lookup_indices(tokens)\n",
    "    return torch.tensor(ids)\n",
    "\n",
    "def build_train_test_data(feature_train, label_train, min_vocab_freq = 2, **kwargs):\n",
    "    train_tokens, train_lengths = build_array(feature_train)\n",
    "\n",
    "    unk_token = '<unk>'\n",
    "    pad_token = '<pad>'\n",
    "    special_tokens = [unk_token, pad_token]\n",
    "\n",
    "    vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "        train_tokens,\n",
    "        min_freq=min_vocab_freq,\n",
    "        specials=special_tokens,\n",
    "    )\n",
    "\n",
    "    unk_id = vocab[unk_token]\n",
    "    pad_id = vocab[unk_token]\n",
    "\n",
    "    vocab.set_default_index(unk_id)\n",
    "\n",
    "    print('vocab len = ', len(vocab))\n",
    "\n",
    "    def convert_to_ids_labels_lengths(token_list, labels, lengths):\n",
    "        id_list = []\n",
    "\n",
    "        for tokens in tqdm(token_list):\n",
    "            ids = get_ids(tokens, vocab)\n",
    "            id_list.append(ids)\n",
    "\n",
    "        #convert y to tensor\n",
    "        labels = torch.tensor(labels)\n",
    "        #convert X lengths to tensor\n",
    "        lengths = torch.tensor(lengths)\n",
    "\n",
    "        return id_list, labels, lengths\n",
    "\n",
    "    train_ids, train_y, train_lengths = convert_to_ids_labels_lengths(train_tokens, label_train, train_lengths)\n",
    "\n",
    "    return (train_tokens, train_ids, train_y, train_lengths), vocab, pad_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df = pd.concat([train_df, test_df])\n",
    "# all_df['text'] = all_df['text'].apply(lambda s: clean_text(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130000/130000 [00:09<00:00, 13250.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len =  800\n",
      "vocab len =  74806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 130000/130000 [00:06<00:00, 19643.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids shape =  torch.Size([130000, 800])\n",
      "y values =  {0, 1, 2, 3, 4}\n",
      "y shape =  torch.Size([130000])\n",
      "lengths shape =  torch.Size([130000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 12529.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len =  800\n",
      "vocab len =  13043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [00:00<00:00, 24049.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids shape =  torch.Size([5000, 800])\n",
      "y values =  {0, 1, 2, 3, 4}\n",
      "y shape =  torch.Size([5000])\n",
      "lengths shape =  torch.Size([5000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_array = train_df['text'].values\n",
    "y_array = train_df['review'].values\n",
    "\n",
    "(train_tokens, train_ids, train_y, train_lengths) ,vocab, pad_id = build_train_test_data(X_array, y_array)\n",
    "\n",
    "train_ids = pad_sequence(train_ids, batch_first=True, padding_value=pad_id)\n",
    "\n",
    "print('ids shape = ', train_ids.shape )\n",
    "print('y values = ', set(train_y.tolist()) )\n",
    "print('y shape = ', train_y.shape )\n",
    "print('lengths shape = ', train_lengths.shape)\n",
    "\n",
    "#Test\n",
    "test_X_array = test_df['text'].values\n",
    "test_y_array = test_df['review'].values\n",
    "\n",
    "(test_tokens, test_ids, test_y, test_lengths) ,_, _ = build_train_test_data(test_X_array, test_y_array)\n",
    "\n",
    "test_ids = pad_sequence(test_ids, batch_first=True, padding_value=pad_id)\n",
    "\n",
    "print('ids shape = ', test_ids.shape )\n",
    "print('y values = ', set(test_y.tolist()) )\n",
    "print('y shape = ', test_y.shape )\n",
    "print('lengths shape = ', test_lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label=1\n",
      "tokens=['ate', 'at', 'wildfish', 'this', 'evening', '.', 'my', 'wife', 'had', 'a', 'really', 'nice', 'salad', '.', 'i', 'ordered', 'the', 'fried', 'oysters', '(', 'good', ')', 'and', 'the', 'asian', 'bbq', 'salmon', '(', 'sauce', 'hid', 'the', 'taste', 'of', 'salmon', 'completely', '.', '\\\\n\\\\nservice', 'was', 'very', 'good', '.', 'atmosphere', 'was', 'dark', '.', '\\\\n\\\\noverall', 'overpriced', '.']\n",
      "length=48\n",
      "ids=tensor([  510,    31, 18064,    24,   676,     2,    18,   438,    30,     7,\n",
      "           67,    97,   199,     2,     6,    98,     3,   331,  1720,    41,\n",
      "           35,    37,     5,     3,   887,   568,   770,    41,   186, 12913,\n",
      "            3,   241,    12,   770,   573,     2,  1037,    10,    50,    35,\n",
      "            2,   337,    10,   893,     2,   791,   701,     2,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "vocab len =  74806\n"
     ]
    }
   ],
   "source": [
    "for label, token, id, length in zip(train_y[:1], train_tokens[:1], train_ids[:1], train_lengths[:1]):\n",
    "    print(f'label={label}\\ntokens={token}\\nlength={length}\\nids={id}')\n",
    "\n",
    "print('vocab len = ', len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, num_hiddens: int, dropout:float = 0, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        assert num_hiddens % 2 == 0, f'num hiddens ({num_hiddens}),has to be even'\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.P = torch.zeros((1, max_len, num_hiddens))\n",
    "        x = torch.arange(max_len).float().unsqueeze(1)\n",
    "        #N = 10000 as defined in the Attention is All You Need paper\n",
    "        denom = torch.pow(10000, torch.arange(0, num_hiddens,2).float()/num_hiddens)\n",
    "        x = x/denom\n",
    "        self.P[:,:,0::2] = torch.sin(x)\n",
    "        self.P[:,:,1::2] = torch.cos(x)\n",
    "\n",
    "        # self.P = self.P.to(device)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # assert x.device == self.P.device, 'in positional encoding, X should have the same device with P'\n",
    "        batch_size, length, num_hiddens = x.shape\n",
    "        x = x + self.P[:,:length,:].to(x.device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_size, num_class, \n",
    "    num_heads, dim_fc, num_tokens, \n",
    "    dropout = 0.2, num_encoder_layers = 2, batch_first = True):\n",
    "\n",
    "        '''\n",
    "        @params:\n",
    "            input_size: input features or embedding size\n",
    "            num_heads: number of multi attentino heads \n",
    "            dim_fc:  dimension of feedforward layer\n",
    "            dropout: drop out rate\n",
    "            num_tokens: number of tokens in vocabulary (some call vocab_size)\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_type = 'Transformer'\n",
    "        self.pos_encoder = PositionalEncoding(input_size, dropout)\n",
    "        self.encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model = input_size,\n",
    "            nhead = num_heads,\n",
    "            dim_feedforward= dim_fc,\n",
    "            dropout = dropout,\n",
    "            batch_first=batch_first,\n",
    "        )\n",
    "\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer=self.encoder_layers, num_layers = num_encoder_layers)\n",
    "        self.embedding = nn.Embedding(num_tokens, input_size)\n",
    "        self.input_size = input_size \n",
    "\n",
    "        self.fc = nn.Linear(input_size, num_class) \n",
    "\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    \n",
    "    def forward(self, src, mask = None):\n",
    "        batch_size, input_len = src.shape\n",
    "        \n",
    "        src = self.embedding(src)\n",
    "        src = src * math.sqrt(self.input_size)\n",
    "        src = self.pos_encoder(src)\n",
    "\n",
    "        if mask == None:\n",
    "            mask = nn.Transformer.generate_square_subsequent_mask(input_len).to(src.device)\n",
    "\n",
    "        transformer_output = self.encoder(src, mask)\n",
    "\n",
    "        # print('transformer output shape = ', transformer_output.shape)\n",
    "        output = self.fc(transformer_output[:,-1]) \n",
    "        # print('fc output shape = ', output.shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = 100 \n",
    "# num_class = 2\n",
    "# num_heads = 5\n",
    "# fc_hidden_size = 5\n",
    "# num_tokens = 1000\n",
    "    \n",
    "# clf = TransformerClassifier(input_size, num_class, num_heads, fc_hidden_size, num_tokens)\n",
    "# clf.to(DEVICE)\n",
    "# # clf.convert_to_device(DEVICE)\n",
    "# X = torch.randint(0,1000,(10,5)).to(DEVICE)\n",
    "\n",
    "# y = clf(X, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpReview(Dataset):\n",
    "\n",
    "    def __init__(self, ids, labels, lengths):\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.ids[idx], self.labels[idx], self.lengths[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.YelpReview object at 0x00000231CFF3EF10>\n",
      "Vocab()\n",
      "74806\n",
      "num classes =  5\n"
     ]
    }
   ],
   "source": [
    "train_dataset = YelpReview(train_ids, train_y, train_lengths)\n",
    "test_dataset = YelpReview(test_ids, train_y, train_lengths)\n",
    "\n",
    "print(train_dataset)\n",
    "print(vocab)\n",
    "print(len(vocab))\n",
    "\n",
    "NUM_CLASSES = len(set(train_dataset.labels.tolist()))\n",
    "print('num classes = ', NUM_CLASSES)\n",
    "\n",
    "# print(train_dataset)\n",
    "\n",
    "# Save the YelpDataset\n",
    "\n",
    "# with open(processed_train_dataset, 'wb') as file:\n",
    "#     pickle.dump(train_dataset, file)\n",
    "\n",
    "# with open(processed_test_dataset, 'wb') as file:\n",
    "#     pickle.dump(test_dataset, file)\n",
    "\n",
    "# with open(vocab_save_path, 'wb') as file:\n",
    "#     pickle.dump(vocab, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# with open(processed_train_dataset, 'rb') as file:\n",
    "#     train_dataset = pickle.load(file)\n",
    "\n",
    "# with open(processed_test_dataset, 'rb') as file:\n",
    "#     test_dataset = pickle.load(file)\n",
    "\n",
    "# with open(vocab_save_path, 'rb') as file:\n",
    "#     vocab = pickle.load(file)\n",
    "\n",
    "# print(train_dataset)\n",
    "# print(vocab)\n",
    "# print(len(vocab))\n",
    "\n",
    "# NUM_CLASSES = len(set(train_dataset.labels.tolist()))\n",
    "# print('num classes = ', NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num class =  5\n",
      "train dataset len =  104000\n",
      "train dataloader len =  1625\n",
      "val dataset len =  26000\n",
      "val dataloader len =  407\n",
      "test dataset len =  5000\n",
      "test dataloader len =  79\n",
      "id shape =  torch.Size([64, 800])\n",
      "label shape =  torch.Size([64])\n",
      "lengths shape =  torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# test model running on dataloader\n",
    "\n",
    "train_ratio = 0.8\n",
    "train_len = int(train_ratio * len(train_dataset))\n",
    "test_val_len = (len(train_dataset) - train_len)\n",
    "train_dataset, val_dataset = random_split(train_dataset,[train_len, test_val_len])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "\n",
    "print('num class = ', NUM_CLASSES)\n",
    "print('train dataset len = ', len(train_dataset))\n",
    "print('train dataloader len = ', len(train_dataloader))\n",
    "print('val dataset len = ', len(val_dataset))\n",
    "print('val dataloader len = ', len(val_dataloader))\n",
    "print('test dataset len = ', len(test_dataset))\n",
    "print('test dataloader len = ', len(test_dataloader))\n",
    "\n",
    "(sample_ids, sample_y, sample_lengths) = next(iter(train_dataloader)) \n",
    "print('id shape = ', sample_ids.shape)\n",
    "print('label shape = ', sample_y.shape)\n",
    "print('lengths shape = ', sample_lengths.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(prediction, label):\n",
    "    batch_size, _ = prediction.shape\n",
    "    predicted_classes = prediction.argmax(dim=-1)\n",
    "    correct_predictions = predicted_classes.eq(label).sum()\n",
    "    accuracy = correct_predictions / batch_size\n",
    "    return accuracy\n",
    "\n",
    "def train(dataloader, model, criterion, optimizer, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "\n",
    "    for ids, label, length in tqdm(dataloader, desc=\"training...\"):\n",
    "        ids = ids.to(device)\n",
    "        label = label.to(device)\n",
    "        length = length\n",
    "        # prediction = model(ids, length)\n",
    "        prediction = model(ids)\n",
    "\n",
    "        loss = criterion(prediction, label)\n",
    "        accuracy = get_accuracy(prediction, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accs.append(accuracy.item())\n",
    "\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs)\n",
    "\n",
    "def pickle_dump(obj, path):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "        \n",
    "def pickle_load(path):\n",
    "    with open(path, 'wb') as file:\n",
    "        obj = pickle.load(file)\n",
    "    return obj\n",
    "\n",
    "def evaluate(dataloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "    \n",
    "    for ids, label, length in tqdm(dataloader, desc=\"evaluating...\"):\n",
    "        ids = ids.to(device)\n",
    "        length = length\n",
    "        label = label.to(device)\n",
    "        # prediction = model(ids, length)\n",
    "        prediction = model(ids)\n",
    "        loss = criterion(prediction, label)\n",
    "        accuracy = get_accuracy(prediction, label)\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accs.append(accuracy.item())\n",
    "            \n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs)\n",
    "\n",
    "def tune(model,train_dataloader, val_dataloader, test_dataloader,  \n",
    "         optimizer, criterion, device, epochs = 10, label = EXP_NAME, history = None):\n",
    "    print(f\"The model has {model.count_parameters()} trainable parameters\")\n",
    "\n",
    "    criterion = criterion.to(device)\n",
    "    best_score = -float(\"inf\")\n",
    "\n",
    "    SAVE_PATH = os.path.join(OUTPUT_PATH, label)\n",
    "    PLOT_PATH = os.path.join(SAVE_PATH, 'plot.png')\n",
    "\n",
    "    if os.path.exists(SAVE_PATH) == False:\n",
    "        os.makedirs(SAVE_PATH)\n",
    "\n",
    "    if history == None:\n",
    "        history = collections.defaultdict(list)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)\n",
    "        val_loss, val_acc = evaluate(val_dataloader, model, criterion, device)\n",
    "        test_loss, test_acc = evaluate(test_dataloader, model, criterion, device)\n",
    "        history[\"train_losses\"].append(train_loss)\n",
    "        history[\"train_accs\"].append(train_acc)\n",
    "        history[\"valid_losses\"].append(val_loss)\n",
    "        history[\"valid_accs\"].append(val_acc)\n",
    "        history[\"test_losses\"].append(test_loss)\n",
    "        history[\"test_accs\"].append(test_acc)\n",
    "\n",
    "        if test_acc > best_score:\n",
    "            best_score = test_loss\n",
    "            torch.save(model, os.path.join(SAVE_PATH, \"transformers.checkpont.torch\"))\n",
    "\n",
    "        print(f\"epoch: {epoch}\")\n",
    "        print(f\"train_loss: {train_loss:.3f}, train_acc: {train_acc:.3f}\")\n",
    "        print(f\"val_loss: {val_loss:.3f}, valid_acc: {val_acc:.3f}\")\n",
    "        print(f\"test_loss: {test_loss:.3f}, test_acc: {test_acc:.3f}\")\n",
    "\n",
    "        plot(history, save_path = os.path.join(SAVE_PATH, 'plot.png'))\n",
    "\n",
    "        pickle_dump(history, os.path.join(SAVE_PATH, 'history.pickle'))\n",
    "\n",
    "    \n",
    "    plot(history, save_path = os.path.join(SAVE_PATH, 'plot.png'), show = True)\n",
    "    return history\n",
    "\n",
    "def plot(history, save_path = None, show = False):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize = (10,5))\n",
    "    epochs = list(range(len(history['train_accs'])))\n",
    "    sns.lineplot(y = history[\"train_accs\"],   label ='train accuracy',  x = epochs, ax = ax1)\n",
    "    sns.lineplot(y = history[\"valid_accs\"],   label ='val accuracy',    x = epochs, ax = ax1)\n",
    "    sns.lineplot(y = history[\"test_accs\"],   label ='test accuracy',    x = epochs, ax = ax1)\n",
    "    ax1.set_title(\"Accuracy\")\n",
    "\n",
    "    sns.lineplot(y = history[\"train_losses\"], label ='train loss', x = epochs, ax = ax2)\n",
    "    sns.lineplot(y = history[\"valid_losses\"],   label ='val loss', x = epochs, ax = ax2)\n",
    "    sns.lineplot(y = history[\"test_losses\"],   label ='test loss', x = epochs, ax = ax2)\n",
    "    ax2.set_title(\"Loss\")\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = torchtext.vocab.GloVe()\n",
    "pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output dim =  5\n",
      "The model has 24454541 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 1625/1625 [07:46<00:00,  3.48it/s]\n",
      "evaluating...: 100%|██████████| 407/407 [00:37<00:00, 10.97it/s]\n",
      "evaluating...: 100%|██████████| 79/79 [00:06<00:00, 11.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "train_loss: 1.169, train_acc: 0.486\n",
      "val_loss: 1.219, valid_acc: 0.545\n",
      "test_loss: 3.716, test_acc: 0.204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 1625/1625 [07:32<00:00,  3.59it/s]\n",
      "evaluating...: 100%|██████████| 407/407 [00:36<00:00, 11.19it/s]\n",
      "evaluating...: 100%|██████████| 79/79 [00:07<00:00, 11.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "train_loss: 1.022, train_acc: 0.554\n",
      "val_loss: 1.243, valid_acc: 0.557\n",
      "test_loss: 3.830, test_acc: 0.201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 1625/1625 [07:34<00:00,  3.58it/s]\n",
      "evaluating...: 100%|██████████| 407/407 [00:36<00:00, 11.07it/s]\n",
      "evaluating...: 100%|██████████| 79/79 [00:06<00:00, 11.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "train_loss: 0.980, train_acc: 0.575\n",
      "val_loss: 1.181, valid_acc: 0.560\n",
      "test_loss: 3.577, test_acc: 0.202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   0%|          | 6/1625 [00:01<08:51,  3.05it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m     24\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 26\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m               \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m plot(history)\n",
      "Cell \u001b[1;32mIn[15], line 77\u001b[0m, in \u001b[0;36mtune\u001b[1;34m(model, train_dataloader, val_dataloader, test_dataloader, optimizer, criterion, device, epochs, label, history)\u001b[0m\n\u001b[0;32m     74\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 77\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate(val_dataloader, model, criterion, device)\n\u001b[0;32m     79\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m evaluate(test_dataloader, model, criterion, device)\n",
      "Cell \u001b[1;32mIn[15], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     22\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m get_accuracy(prediction, label)\n\u001b[0;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 24\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     26\u001b[0m epoch_losses\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 5e-4\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 300\n",
    "hidden_dim = 300\n",
    "output_dim = NUM_CLASSES\n",
    "print('output dim = ', output_dim)\n",
    "n_layers = 2\n",
    "bidirectional = True\n",
    "dropout_rate = 0.5\n",
    "epochs = 10\n",
    "\n",
    "model = TransformerClassifier(\n",
    "    input_size = embedding_dim,\n",
    "    num_class=  NUM_CLASSES,\n",
    "    num_heads = 5,\n",
    "    dim_fc = 512,\n",
    "    num_tokens = vocab_size,\n",
    "    dropout = dropout_rate\n",
    ")\n",
    "\n",
    "model.embedding.weight.data = pretrained_embedding\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history = tune(model,  train_dataloader, val_dataloader, test_dataloader, \n",
    "               optimizer, criterion, epochs = epochs, device = DEVICE)\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "test accuracy not improving. Perhaps the train vocab and test vocab is too different. Try merging train and test tokens. See what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
