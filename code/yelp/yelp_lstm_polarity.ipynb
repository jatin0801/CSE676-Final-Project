{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nguye\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device =  cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import re\n",
    "import contractions\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "import torchtext\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, random_split \n",
    "from torch import nn\n",
    "import torch \n",
    "\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print('device = ', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_bb5b9cdc\n"
     ]
    }
   ],
   "source": [
    "# root = os.getcwd()\n",
    "# dataset =  os.path.join(root, 'dataset', 'amazon_full')\n",
    "# #yelp polarity has 2 labels, \n",
    "# #yelp dataset has 5 labels, they have the same text content\n",
    "# train_datapath = os.path.join(dataset, 'train.csv') \n",
    "# test_datapath = os.path.join(dataset, 'test.csv') \n",
    "\n",
    "# processed_train_dataset = os.path.join(dataset, 'processed_train_datset.pickle')\n",
    "# processed_test_dataset = os.path.join(dataset, 'processed_test_dataset.pickle')\n",
    "# vocab_save_path = os.path.join(dataset, 'vocab.pickle')\n",
    "\n",
    "ROOT = os.getcwd()\n",
    "DATASET_PATH = os.path.join(ROOT, 'dataset', 'yelp_polarity' )\n",
    "\n",
    "CLEANED_TRAIN_DATAPATH = os.path.join(DATASET_PATH, 'cleaned_train_20%.csv')\n",
    "CLEANED_TEST_DATAPATH = os.path.join(DATASET_PATH, 'cleaned_test_10%.csv')\n",
    "\n",
    "# model_save_root = os.path.join(root, 'output', 'transformer')\n",
    "\n",
    "OUTPUT_PATH = os.path.join(ROOT, 'output', 'lstm')\n",
    "\n",
    "if os.path.exists(OUTPUT_PATH) == False:\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "\n",
    "id = str(uuid.uuid4()).split('-')[0]\n",
    "EXP_NAME = f'baseline_{id}'\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "MAX_SEQUENCE_LENGTH = 512\n",
    "\n",
    "print(EXP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test string special characters punctuations _'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning Text\n",
    "\n",
    "def remove_urls(text):\n",
    "    #if there's link in text, like www.something.com, https://www.something.com,\n",
    "    # replace it with the <url> token\n",
    "    pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text = pattern.sub(' ', text)\n",
    "    return text\n",
    "\n",
    "def remove_digits(text):\n",
    "    return re.sub(\"\\d\", ' ', text)\n",
    "\n",
    "def remove_punctation(text):\n",
    "    return re.sub(r'[^\\w\\s]',' ',text)\n",
    "\n",
    "def expand_contraction(text):\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return ' '.join([word for word in text.split(' ') if word not in STOPWORDS])\n",
    "\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    extract feature and label from line and process the text\n",
    "    @params:\n",
    "        text: string, format: __label__2 some text.\n",
    "    @return:\n",
    "        feature: string\n",
    "        label: int, 0: bad review, 1 good review\n",
    "    '''\n",
    "    #Each line has format: __label__2 some text.\n",
    "    #The first part is label, the rest is text feature\n",
    "    #lower case the features\n",
    "    text = text.lower()\n",
    "    #start cleaning\n",
    "\n",
    "    #remove urls in text\n",
    "    text = remove_urls(text)\n",
    "    #remove digits\n",
    "    text = remove_digits(text)\n",
    "    # # #expand contractions\n",
    "    text = expand_contraction(text)\n",
    "    # # #remove punctuations\n",
    "    text = remove_punctation(text)\n",
    "    # # #remove stop words\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "    #after cleaning, there's a letter n that occur most frequently\n",
    "    #this don't make sense so remove a standalone letter n\n",
    "    text = ' '.join(t for t in text.split() if t != '' and t != 'n')\n",
    "    return text.strip()\n",
    "\n",
    "test_string = '''This is a test string. Here are some special characters: &,#,$. How about some punctuations? !@#$%^&*()_+=-`~{[]}|:;'<,>.?/\"|https://www.example.com'''\n",
    "\n",
    "clean_text(test_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(CLEANED_TRAIN_DATAPATH)\n",
    "NUM_CLASSES = len(train_df['review'].unique())\n",
    "test_df = pd.read_csv(CLEANED_TEST_DATAPATH)\n",
    "print(NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = torchtext.data.utils.get_tokenizer(\"basic_english\")\n",
    "\n",
    "def build_array(X, maxlength = 800):\n",
    "    X_tokens = []\n",
    "    X_lengths = []\n",
    "    max_len = 0\n",
    "    for text in tqdm(X):\n",
    "        tokens = tokenizer(text)[:maxlength]\n",
    "        max_len = max(max_len, len(tokens))\n",
    "        X_tokens.append(tokens)\n",
    "        X_lengths.append(len(tokens))\n",
    "\n",
    "    print('max len = ', max_len)\n",
    "    return X_tokens, X_lengths\n",
    "\n",
    "\n",
    "def get_ids(tokens, vocab):\n",
    "    ids = vocab.lookup_indices(tokens)\n",
    "    return torch.tensor(ids)\n",
    "\n",
    "def build_train_test_data(feature_train, label_train, min_vocab_freq = 2, **kwargs):\n",
    "    train_tokens, train_lengths = build_array(feature_train)\n",
    "\n",
    "    unk_token = '<unk>'\n",
    "    pad_token = '<pad>'\n",
    "    special_tokens = [unk_token, pad_token]\n",
    "\n",
    "    vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "        train_tokens,\n",
    "        min_freq=min_vocab_freq,\n",
    "        specials=special_tokens,\n",
    "    )\n",
    "\n",
    "    unk_id = vocab[unk_token]\n",
    "    pad_id = vocab[unk_token]\n",
    "\n",
    "    vocab.set_default_index(unk_id)\n",
    "\n",
    "    print('vocab len = ', len(vocab))\n",
    "\n",
    "    def convert_to_ids_labels_lengths(token_list, labels, lengths):\n",
    "        id_list = []\n",
    "\n",
    "        for tokens in tqdm(token_list):\n",
    "            ids = get_ids(tokens, vocab)\n",
    "            id_list.append(ids)\n",
    "\n",
    "        #convert y to tensor\n",
    "        labels = torch.tensor(labels)\n",
    "        #convert X lengths to tensor\n",
    "        lengths = torch.tensor(lengths)\n",
    "\n",
    "        return id_list, labels, lengths\n",
    "\n",
    "    train_ids, train_y, train_lengths = convert_to_ids_labels_lengths(train_tokens, label_train, train_lengths)\n",
    "\n",
    "    return (train_tokens, train_ids, train_y, train_lengths), vocab, pad_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_df = pd.concat([train_df, test_df])\n",
    "# all_df['text'] = all_df['text'].apply(lambda s: clean_text(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112000/112000 [00:07<00:00, 14343.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len =  800\n",
      "vocab len =  68665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 112000/112000 [00:04<00:00, 23377.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids shape =  torch.Size([112000, 800])\n",
      "y values =  {0, 1}\n",
      "y shape =  torch.Size([112000])\n",
      "lengths shape =  torch.Size([112000])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3800/3800 [00:00<00:00, 13975.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len =  800\n",
      "vocab len =  11280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3800/3800 [00:00<00:00, 24203.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids shape =  torch.Size([3800, 800])\n",
      "y values =  {0, 1}\n",
      "y shape =  torch.Size([3800])\n",
      "lengths shape =  torch.Size([3800])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_array = train_df['text'].values\n",
    "y_array = train_df['review'].values\n",
    "\n",
    "(train_tokens, train_ids, train_y, train_lengths) ,vocab, pad_id = build_train_test_data(X_array, y_array)\n",
    "\n",
    "train_ids = pad_sequence(train_ids, batch_first=True, padding_value=pad_id)\n",
    "\n",
    "print('ids shape = ', train_ids.shape )\n",
    "print('y values = ', set(train_y.tolist()) )\n",
    "print('y shape = ', train_y.shape )\n",
    "print('lengths shape = ', train_lengths.shape)\n",
    "\n",
    "#Test\n",
    "test_X_array = test_df['text'].values\n",
    "test_y_array = test_df['review'].values\n",
    "\n",
    "(test_tokens, test_ids, test_y, test_lengths) ,_, _ = build_train_test_data(test_X_array, test_y_array)\n",
    "\n",
    "test_ids = pad_sequence(test_ids, batch_first=True, padding_value=pad_id)\n",
    "\n",
    "print('ids shape = ', test_ids.shape )\n",
    "print('y values = ', set(test_y.tolist()) )\n",
    "print('y shape = ', test_y.shape )\n",
    "print('lengths shape = ', test_lengths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label=1\n",
      "tokens=['tacky', 'name', ',', 'nice', 'hotel', '.', 'definately', 'didn', \"'\", 't', 'know', 'what', 'i', 'was', 'getting', 'but', 'it', 'was', 'cute', 'and', 'i', 'was', 'pleasantly', 'surprised', '.', 'it', \"'\", 's', 'on', 'fremont', 'a', 'kind', 'of', 'cool', 'area', 'but', 'you', 'won', \"'\", 't', 'find', 'as', 'many', 'young', 'people', 'in', 'my', 'opinion', '.', 'the', 'pool', 'area', 'is', 'pretty', 'cool', 'they', 'have', 'this', 'slide', 'it', \"'\", 's', 'clear', 'and', 'you', 'slide', 'through', 'sharks', 'and', 'fish', 'it', \"'\", 's', 'really', 'neat', '.', 'definately', 'a', 'good', 'budget', 'hotel']\n",
      "length=81\n",
      "ids=tensor([ 3720,   450,     4,   103,   221,     2,  4476,    95,     9,    25,\n",
      "          121,    70,     6,    10,   288,    23,    11,    10,   882,     5,\n",
      "            6,    10,  2023,   754,     2,    11,     9,    29,    26,  3235,\n",
      "            7,   328,    12,   415,   213,    23,    20,   344,     9,    25,\n",
      "          196,    43,   200,   959,   104,    14,    18,   999,     2,     3,\n",
      "          522,   213,    15,   134,   415,    21,    28,    22,  3728,    11,\n",
      "            9,    29,  1070,     5,    20,  3728,   305, 11006,     5,   396,\n",
      "           11,     9,    29,    72,  2685,     2,  4476,     7,    41,  2135,\n",
      "          221,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "vocab len =  68665\n"
     ]
    }
   ],
   "source": [
    "for label, token, id, length in zip(train_y[:1], train_tokens[:1], train_ids[:1], train_lengths[:1]):\n",
    "    print(f'label={label}\\ntokens={token}\\nlength={length}\\nids={id}')\n",
    "\n",
    "print('vocab len = ', len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(\n",
    "        self,vocab_size,embedding_dim,hidden_dim,output_dim,n_layers,\n",
    "        bidirectional,dropout_rate,pad_id,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_id)\n",
    "        self.lstm = nn.LSTM(embedding_dim,hidden_dim,n_layers,bidirectional=bidirectional,dropout=dropout_rate,batch_first=True,)\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "    \n",
    "    def count_parameters(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def forward(self, ids, length):\n",
    "        embedded = self.dropout(self.embedding(ids))\n",
    "        embedded = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, length, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        output, (hidden, cell) = self.lstm(embedded)\n",
    "        if self.lstm.bidirectional:\n",
    "            hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1])\n",
    "        prediction = self.fc(hidden)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = 100 \n",
    "# num_class = 2\n",
    "# num_heads = 5\n",
    "# fc_hidden_size = 5\n",
    "# num_tokens = 1000\n",
    "    \n",
    "# clf = TransformerClassifier(input_size, num_class, num_heads, fc_hidden_size, num_tokens)\n",
    "# clf.to(DEVICE)\n",
    "# # clf.convert_to_device(DEVICE)\n",
    "# X = torch.randint(0,1000,(10,5)).to(DEVICE)\n",
    "\n",
    "# y = clf(X, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YelpReview(Dataset):\n",
    "\n",
    "    def __init__(self, ids, labels, lengths):\n",
    "        self.ids = ids\n",
    "        self.labels = labels\n",
    "        self.lengths = lengths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.ids[idx], self.labels[idx], self.lengths[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.YelpReview object at 0x00000251BBD91940>\n",
      "Vocab()\n",
      "68665\n",
      "num classes =  2\n"
     ]
    }
   ],
   "source": [
    "train_dataset = YelpReview(train_ids, train_y, train_lengths)\n",
    "test_dataset = YelpReview(test_ids, test_y, test_lengths)\n",
    "\n",
    "print(train_dataset)\n",
    "print(vocab)\n",
    "print(len(vocab))\n",
    "\n",
    "NUM_CLASSES = len(set(train_dataset.labels.tolist()))\n",
    "print('num classes = ', NUM_CLASSES)\n",
    "\n",
    "# print(train_dataset)\n",
    "\n",
    "# Save the YelpDataset\n",
    "\n",
    "# with open(processed_train_dataset, 'wb') as file:\n",
    "#     pickle.dump(train_dataset, file)\n",
    "\n",
    "# with open(processed_test_dataset, 'wb') as file:\n",
    "#     pickle.dump(test_dataset, file)\n",
    "\n",
    "# with open(vocab_save_path, 'wb') as file:\n",
    "#     pickle.dump(vocab, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num class =  2\n",
      "train dataset len =  89600\n",
      "train dataloader len =  1400\n",
      "val dataset len =  22400\n",
      "val dataloader len =  350\n",
      "test dataset len =  3800\n",
      "test dataloader len =  60\n",
      "id shape =  torch.Size([64, 800])\n",
      "label shape =  torch.Size([64])\n",
      "lengths shape =  torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# test model running on dataloader\n",
    "\n",
    "train_ratio = 0.8\n",
    "train_len = int(train_ratio * len(train_dataset))\n",
    "test_val_len = (len(train_dataset) - train_len)\n",
    "train_dataset, val_dataset = random_split(train_dataset,[train_len, test_val_len])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle = True)\n",
    "\n",
    "print('num class = ', NUM_CLASSES)\n",
    "print('train dataset len = ', len(train_dataset))\n",
    "print('train dataloader len = ', len(train_dataloader))\n",
    "print('val dataset len = ', len(val_dataset))\n",
    "print('val dataloader len = ', len(val_dataloader))\n",
    "print('test dataset len = ', len(test_dataset))\n",
    "print('test dataloader len = ', len(test_dataloader))\n",
    "\n",
    "(sample_ids, sample_y, sample_lengths) = next(iter(train_dataloader)) \n",
    "print('id shape = ', sample_ids.shape)\n",
    "print('label shape = ', sample_y.shape)\n",
    "print('lengths shape = ', sample_lengths.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(prediction, label):\n",
    "    batch_size, _ = prediction.shape\n",
    "    predicted_classes = prediction.argmax(dim=-1)\n",
    "    correct_predictions = predicted_classes.eq(label).sum()\n",
    "    accuracy = correct_predictions / batch_size\n",
    "    return accuracy\n",
    "\n",
    "def train(dataloader, model, criterion, optimizer, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "\n",
    "    for ids, label, length in tqdm(dataloader, desc=\"training...\"):\n",
    "        ids = ids.to(device)\n",
    "        label = label.to(device)\n",
    "        length = length\n",
    "        prediction = model(ids, length)\n",
    "\n",
    "        loss = criterion(prediction, label)\n",
    "        accuracy = get_accuracy(prediction, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accs.append(accuracy.item())\n",
    "\n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs)\n",
    "\n",
    "def pickle_dump(obj, path):\n",
    "    with open(path, 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "        \n",
    "def pickle_load(path):\n",
    "    with open(path, 'wb') as file:\n",
    "        obj = pickle.load(file)\n",
    "    return obj\n",
    "\n",
    "def evaluate(dataloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_losses = []\n",
    "    epoch_accs = []\n",
    "    \n",
    "    for ids, label, length in tqdm(dataloader, desc=\"evaluating...\"):\n",
    "        ids = ids.to(device)\n",
    "        length = length\n",
    "        label = label.to(device)\n",
    "        # prediction = model(ids, length)\n",
    "        prediction = model(ids, length)\n",
    "        loss = criterion(prediction, label)\n",
    "        accuracy = get_accuracy(prediction, label)\n",
    "        epoch_losses.append(loss.item())\n",
    "        epoch_accs.append(accuracy.item())\n",
    "            \n",
    "    return np.mean(epoch_losses), np.mean(epoch_accs)\n",
    "\n",
    "def tune(model,train_dataloader, val_dataloader, test_dataloader,  \n",
    "         optimizer, criterion, device, epochs = 10, label = EXP_NAME, history = None):\n",
    "    print(f\"The model has {model.count_parameters()} trainable parameters\")\n",
    "\n",
    "    criterion = criterion.to(device)\n",
    "    best_score = -float(\"inf\")\n",
    "\n",
    "    SAVE_PATH = os.path.join(OUTPUT_PATH, label)\n",
    "    PLOT_PATH = os.path.join(SAVE_PATH, 'plot.png')\n",
    "\n",
    "    if os.path.exists(SAVE_PATH) == False:\n",
    "        os.makedirs(SAVE_PATH)\n",
    "\n",
    "    if history == None:\n",
    "        history = collections.defaultdict(list)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_acc = train(train_dataloader, model, criterion, optimizer, device)\n",
    "        val_loss, val_acc = evaluate(val_dataloader, model, criterion, device)\n",
    "        test_loss, test_acc = evaluate(test_dataloader, model, criterion, device)\n",
    "        history[\"train_losses\"].append(train_loss)\n",
    "        history[\"train_accs\"].append(train_acc)\n",
    "        history[\"valid_losses\"].append(val_loss)\n",
    "        history[\"valid_accs\"].append(val_acc)\n",
    "        history[\"test_losses\"].append(test_loss)\n",
    "        history[\"test_accs\"].append(test_acc)\n",
    "\n",
    "        if test_acc > best_score:\n",
    "            best_score = test_loss\n",
    "            torch.save(model, os.path.join(SAVE_PATH, \"transformers.checkpont.torch\"))\n",
    "\n",
    "        print(f\"epoch: {epoch}\")\n",
    "        print(f\"train_loss: {train_loss:.3f}, train_acc: {train_acc:.3f}\")\n",
    "        print(f\"val_loss: {val_loss:.3f}, valid_acc: {val_acc:.3f}\")\n",
    "        print(f\"test_loss: {test_loss:.3f}, test_acc: {test_acc:.3f}\")\n",
    "\n",
    "        plot(history, save_path = os.path.join(SAVE_PATH, 'plot.png'))\n",
    "\n",
    "        pickle_dump(history, os.path.join(SAVE_PATH, 'history.pickle'))\n",
    "\n",
    "    \n",
    "    plot(history, save_path = os.path.join(SAVE_PATH, 'plot.png'), show = True)\n",
    "    return history\n",
    "\n",
    "def plot(history, save_path = None, show = False):\n",
    "    fig, (ax1, ax2) = plt.subplots(1,2, figsize = (10,5))\n",
    "    epochs = list(range(len(history['train_accs'])))\n",
    "    sns.lineplot(y = history[\"train_accs\"],   label ='train accuracy',  x = epochs, ax = ax1)\n",
    "    sns.lineplot(y = history[\"valid_accs\"],   label ='val accuracy',    x = epochs, ax = ax1)\n",
    "    sns.lineplot(y = history[\"test_accs\"],   label ='test accuracy',    x = epochs, ax = ax1)\n",
    "    ax1.set_title(\"Accuracy\")\n",
    "\n",
    "    sns.lineplot(y = history[\"train_losses\"], label ='train loss', x = epochs, ax = ax2)\n",
    "    sns.lineplot(y = history[\"valid_losses\"],   label ='val loss', x = epochs, ax = ax2)\n",
    "    sns.lineplot(y = history[\"test_losses\"],   label ='test loss', x = epochs, ax = ax2)\n",
    "    ax2.set_title(\"Loss\")\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pretrained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = torchtext.vocab.GloVe()\n",
    "pretrained_embedding = vectors.get_vecs_by_tokens(vocab.get_itos())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output dim =  2\n",
      "The model has 36534990 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 1400/1400 [08:53<00:00,  2.62it/s]\n",
      "evaluating...: 100%|██████████| 350/350 [00:48<00:00,  7.27it/s]\n",
      "evaluating...: 100%|██████████| 60/60 [00:08<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "train_loss: 0.294, train_acc: 0.875\n",
      "val_loss: 0.177, valid_acc: 0.932\n",
      "test_loss: 0.919, test_acc: 0.636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 1400/1400 [08:56<00:00,  2.61it/s]\n",
      "evaluating...: 100%|██████████| 350/350 [00:48<00:00,  7.25it/s]\n",
      "evaluating...: 100%|██████████| 60/60 [00:08<00:00,  7.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "train_loss: 0.153, train_acc: 0.943\n",
      "val_loss: 0.146, valid_acc: 0.946\n",
      "test_loss: 0.926, test_acc: 0.617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 1400/1400 [09:06<00:00,  2.56it/s]\n",
      "evaluating...: 100%|██████████| 350/350 [00:50<00:00,  6.88it/s]\n",
      "evaluating...: 100%|██████████| 60/60 [00:08<00:00,  6.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2\n",
      "train_loss: 0.111, train_acc: 0.960\n",
      "val_loss: 0.136, valid_acc: 0.950\n",
      "test_loss: 0.978, test_acc: 0.615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 1400/1400 [09:11<00:00,  2.54it/s]\n",
      "evaluating...: 100%|██████████| 350/350 [00:46<00:00,  7.56it/s]\n",
      "evaluating...: 100%|██████████| 60/60 [00:08<00:00,  7.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3\n",
      "train_loss: 0.079, train_acc: 0.972\n",
      "val_loss: 0.151, valid_acc: 0.947\n",
      "test_loss: 1.214, test_acc: 0.608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...: 100%|██████████| 1400/1400 [08:35<00:00,  2.72it/s]\n",
      "evaluating...: 100%|██████████| 350/350 [00:47<00:00,  7.32it/s]\n",
      "evaluating...: 100%|██████████| 60/60 [00:08<00:00,  7.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4\n",
      "train_loss: 0.055, train_acc: 0.982\n",
      "val_loss: 0.158, valid_acc: 0.947\n",
      "test_loss: 1.313, test_acc: 0.599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training...:   0%|          | 6/1400 [00:02<08:39,  2.68it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[0;32m     26\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m---> 28\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtune\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m               \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m plot(history)\n",
      "Cell \u001b[1;32mIn[14], line 76\u001b[0m, in \u001b[0;36mtune\u001b[1;34m(model, train_dataloader, val_dataloader, test_dataloader, optimizer, criterion, device, epochs, label, history)\u001b[0m\n\u001b[0;32m     73\u001b[0m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 76\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate(val_dataloader, model, criterion, device)\n\u001b[0;32m     78\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m evaluate(test_dataloader, model, criterion, device)\n",
      "Cell \u001b[1;32mIn[14], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     16\u001b[0m label \u001b[38;5;241m=\u001b[39m label\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m length \u001b[38;5;241m=\u001b[39m length\n\u001b[1;32m---> 18\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(prediction, label)\n\u001b[0;32m     21\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m get_accuracy(prediction, label)\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[9], line 25\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, ids, length)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, ids, length):\n\u001b[0;32m     24\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(ids))\n\u001b[1;32m---> 25\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpack_padded_sequence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43membedded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_sorted\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     output, (hidden, cell) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm(embedded)\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm\u001b[38;5;241m.\u001b[39mbidirectional:\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\utils\\rnn.py:263\u001b[0m, in \u001b[0;36mpack_padded_sequence\u001b[1;34m(input, lengths, batch_first, enforce_sorted)\u001b[0m\n\u001b[0;32m    259\u001b[0m     batch_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_first \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mindex_select(batch_dim, sorted_indices)\n\u001b[0;32m    262\u001b[0m data, batch_sizes \u001b[38;5;241m=\u001b[39m \\\n\u001b[1;32m--> 263\u001b[0m     \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pack_padded_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlengths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _packed_sequence_init(data, batch_sizes, sorted_indices, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 5e-4\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 300\n",
    "hidden_dim = 512\n",
    "output_dim = NUM_CLASSES\n",
    "print('output dim = ', output_dim)\n",
    "n_layers = 3\n",
    "bidirectional = True\n",
    "dropout_rate = 0.2\n",
    "epochs = 10\n",
    "\n",
    "model = LSTM(\n",
    "    vocab_size,\n",
    "    embedding_dim,\n",
    "    hidden_dim,\n",
    "    output_dim,\n",
    "    n_layers,\n",
    "    bidirectional=True,\n",
    "    dropout_rate=dropout_rate,\n",
    "    pad_id = pad_id\n",
    ")\n",
    "\n",
    "model.embedding.weight.data = pretrained_embedding\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "history = tune(model,  train_dataloader, val_dataloader, test_dataloader, \n",
    "               optimizer, criterion, epochs = 5, device = DEVICE)\n",
    "\n",
    "plot(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notes\n",
    "\n",
    "test accuracy not improving. Perhaps the train vocab and test vocab is too different. Try merging train and test tokens. See what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.18 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af18273774455bc90f5456b9f4898eab7ba4de506fde0c1d0784da333c7e8bbc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
